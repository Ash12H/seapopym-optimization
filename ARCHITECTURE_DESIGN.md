# Architecture Design - Refactoring GeneticAlgorithm

## üéØ Objectifs

Refactorer `GeneticAlgorithm` pour :
- **S√©parer les responsabilit√©s** (GA logique vs Distribution vs √âvaluation)
- **Am√©liorer la lisibilit√©** pour les utilisateurs m√©tier
- **Faciliter la testabilit√©** avec isolation des composants
- **Permettre l'extensibilit√©** vers d'autres backends de calcul

## üîç Probl√®me Actuel

La classe `GeneticAlgorithm` actuelle viole le **Single Responsibility Principle** :

```python
class GeneticAlgorithm:
    # ‚úì Responsabilit√©: Logique GA
    def optimize(self):
        for generation in range(self.NGEN):
            # ... logique GA ...

    # ‚ùå Responsabilit√©: Distribution Dask
    def distribute_data(self):
        self._distributed_forcing = client.scatter(...)

    # ‚ùå Responsabilit√©: Strat√©gie d'√©valuation
    def _evaluate(self):
        if self.client is None:
            # Mode s√©quentiel
        else:
            # Mode parall√®le
```

**Cons√©quences** :
- Code complexe et difficile √† maintenir
- Tests difficiles (couplage fort)
- Extensibilit√© limit√©e (nouveaux backends = refactoring massif)
- Lisibilit√© r√©duite pour les utilisateurs m√©tier

## üèóÔ∏è Nouvelle Architecture : Composition + Strategy Pattern

### 1. Gestionnaire de Distribution

```python
class DistributionManager:
    """
    Responsabilit√©: Gestion de la distribution des donn√©es avec Dask.
    Encapsule toute la complexit√© li√©e √† Dask.
    """

    def __init__(self, client: Client):
        self.client = client
        self._distributed_data = {}
        self._original_data = {}

    def distribute_forcing(self, forcing_parameters) -> object:
        """
        Distribue les param√®tres de for√ßage avec broadcast=True.

        Returns
        -------
        object
            Future Dask distribu√©e
        """
        if 'forcing' in self._distributed_data:
            warnings.warn("Forcing parameters d√©j√† distribu√©s", UserWarning)
            return self._distributed_data['forcing']

        logger.info("Distribution des param√®tres de for√ßage...")
        scattered = self.client.scatter(forcing_parameters, broadcast=True)
        self._distributed_data['forcing'] = scattered
        self._original_data['forcing'] = forcing_parameters

        return scattered

    def distribute_observations(self, observations) -> list:
        """
        Distribue les observations avec broadcast=True.

        Parameters
        ----------
        observations : list[ObservationProtocol]
            Liste des observations √† distribuer

        Returns
        -------
        list[object]
            Liste des Futures Dask distribu√©es
        """
        if 'observations' in self._distributed_data:
            warnings.warn("Observations d√©j√† distribu√©es", UserWarning)
            return self._distributed_data['observations']

        logger.info("Distribution des observations...")
        scattered_obs = []
        for obs in observations:
            scattered = self.client.scatter(obs.observation, broadcast=True)
            scattered_obs.append(scattered)

        self._distributed_data['observations'] = scattered_obs
        self._original_data['observations'] = observations

        return scattered_obs

    def create_distributed_evaluator(self, cost_function) -> callable:
        """
        Cr√©e une fonction d'√©valuation utilisant les donn√©es distribu√©es.

        Parameters
        ----------
        cost_function : CostFunctionProtocol
            Fonction de co√ªt √† adapter pour la distribution

        Returns
        -------
        callable
            Fonction d'√©valuation distribu√©e
        """
        forcing_future = self._distributed_data.get('forcing')
        obs_futures = self._distributed_data.get('observations', [])

        if not forcing_future or not obs_futures:
            raise RuntimeError("Donn√©es non distribu√©es. Appelez distribute_* d'abord.")

        return partial(
            distributed_evaluate,
            forcing_future,
            obs_futures,
            cost_function.functional_groups,
            cost_function.evaluation_function
        )

    def cleanup(self):
        """Nettoie les r√©f√©rences aux Futures distribu√©es."""
        self._distributed_data.clear()
        self._original_data.clear()


def distributed_evaluate(forcing_future, observations_futures, functional_groups,
                        evaluation_function, individual_params):
    """
    Fonction d'√©valuation distribu√©e.
    Les Futures sont r√©solues automatiquement par Dask comme arguments directs.

    Parameters
    ----------
    forcing_future : ForcingParameter
        Param√®tres de for√ßage (Future r√©solue automatiquement)
    observations_futures : list[xr.DataArray]
        Observations (Futures r√©solues automatiquement)
    functional_groups : FunctionalGroupSet
        Configuration des groupes fonctionnels
    evaluation_function : callable
        Fonction d'√©valuation originale
    individual_params : list[float]
        Param√®tres de l'individu √† √©valuer

    Returns
    -------
    tuple
        Fitness de l'individu
    """
    # Cr√©er un model_generator temporaire avec les donn√©es r√©solues
    from seapopym_optimization.model_generator import NoTransportModelGenerator
    temp_model_generator = NoTransportModelGenerator(forcing_parameters=forcing_future)

    # Cr√©er des observations temporaires avec les donn√©es r√©solues
    temp_observations = []
    for obs_data in observations_futures:
        # Utiliser le type original mais avec les donn√©es r√©solues
        # Note: N√©cessite acc√®s aux m√©tadonn√©es originales
        temp_obs = create_observation_from_data(obs_data)
        temp_observations.append(temp_obs)

    # Cr√©er une cost_function temporaire
    from seapopym_optimization.cost_function.cost_function import CostFunction
    temp_cost_function = CostFunction(
        model_generator=temp_model_generator,
        observations=temp_observations,
        functional_groups=functional_groups,
        evaluation_function=evaluation_function
    )

    # √âvaluer avec la cost function temporaire
    return temp_cost_function.generate()(individual_params)
```

### 2. Strat√©gies d'√âvaluation

```python
from abc import ABC, abstractmethod

class EvaluationStrategy(ABC):
    """Interface pour les diff√©rentes strat√©gies d'√©valuation."""

    @abstractmethod
    def evaluate(self, individuals: list, toolbox: base.Toolbox) -> list:
        """
        √âvalue une liste d'individus.

        Parameters
        ----------
        individuals : list
            Liste des individus √† √©valuer
        toolbox : base.Toolbox
            Toolbox DEAP avec la fonction d'√©valuation

        Returns
        -------
        list
            Liste des fitness calcul√©es
        """
        pass


class SequentialEvaluation(EvaluationStrategy):
    """Strat√©gie d'√©valuation s√©quentielle classique."""

    def evaluate(self, individuals: list, toolbox: base.Toolbox) -> list:
        """√âvaluation s√©quentielle avec map() standard."""
        return list(map(toolbox.evaluate, individuals))


class DistributedEvaluation(EvaluationStrategy):
    """Strat√©gie d'√©valuation distribu√©e avec Dask."""

    def __init__(self, distribution_manager: DistributionManager):
        self.distribution_manager = distribution_manager

    def evaluate(self, individuals: list, toolbox: base.Toolbox) -> list:
        """
        √âvaluation distribu√©e utilisant client.map() avec donn√©es pr√©-distribu√©es.
        """
        # Cr√©er la fonction d'√©valuation distribu√©e
        distributed_evaluator = self.distribution_manager.create_distributed_evaluator(
            toolbox.cost_function
        )

        # Mapper sur les workers avec les Futures comme arguments directs
        individual_params = [list(ind) for ind in individuals]
        futures = self.distribution_manager.client.map(
            distributed_evaluator,
            individual_params
        )

        # Collecter les r√©sultats
        return self.distribution_manager.client.gather(futures)


class ParallelEvaluation(EvaluationStrategy):
    """Strat√©gie d'√©valuation parall√®le classique (multiprocessing)."""

    def __init__(self, n_jobs: int = -1):
        self.n_jobs = n_jobs

    def evaluate(self, individuals: list, toolbox: base.Toolbox) -> list:
        """√âvaluation parall√®le avec multiprocessing."""
        from concurrent.futures import ProcessPoolExecutor

        with ProcessPoolExecutor(max_workers=self.n_jobs) as executor:
            futures = [executor.submit(toolbox.evaluate, ind) for ind in individuals]
            return [future.result() for future in futures]
```

### 3. Classe GeneticAlgorithm Simplifi√©e

```python
@dataclass
class GeneticAlgorithm:
    """
    Algorithme g√©n√©tique focalis√© sur la logique m√©tier.

    D√©l√®gue l'√©valuation √† une strat√©gie configurable, permettant
    diff√©rents modes d'ex√©cution sans complexifier la logique principale.
    """

    meta_parameter: GeneticAlgorithmParameters
    cost_function: CostFunctionProtocol
    evaluation_strategy: EvaluationStrategy = field(default_factory=SequentialEvaluation)
    constraint: Sequence[ConstraintProtocol] | None = None
    save: FilePath | WriteBuffer[bytes] | None = None
    logbook: OptimizationLog | None = field(default=None, repr=False)
    toolbox: base.Toolbox | None = field(default=None, init=False, repr=False)

    def __post_init__(self: GeneticAlgorithm) -> None:
        """Initialisation focalis√©e sur la logique GA."""
        # Configuration du logbook
        if self.save is not None:
            self.save = Path(self.save)
            if self.save.exists():
                logger.warning(f"Logbook file {self.save} already exists. It will be overwritten.")

        # G√©n√©ration du toolbox
        ordered_parameters = self.cost_function.functional_groups.unique_functional_groups_parameters_ordered()
        self.toolbox = self.meta_parameter.generate_toolbox(ordered_parameters.values(), self.cost_function)

        # Application des contraintes
        if self.constraint is not None:
            for constraint in self.constraint:
                self.toolbox.decorate("evaluate", constraint.generate(list(ordered_parameters.keys())))

        # Validation des poids
        if len(self.meta_parameter.cost_function_weight) != len(self.cost_function.observations):
            msg = (
                "The cost function weight must have the same length as the number of observations. "
                f"Got {len(self.meta_parameter.cost_function_weight)} and {len(self.cost_function.observations)}."
            )
            raise ValueError(msg)

    def _evaluate(self: GeneticAlgorithm, individuals: Sequence, generation: int) -> OptimizationLog:
        """
        √âvalue les individus en d√©l√©guant √† la strat√©gie d'√©valuation.
        Logique simplifi√©e et focalis√©e.
        """
        def update_fitness(individuals: list) -> list:
            known = [ind.fitness.valid for ind in individuals]
            invalid_ind = [ind for ind in individuals if not ind.fitness.valid]

            if invalid_ind:
                # D√©l√©gation √† la strat√©gie d'√©valuation
                fitnesses = self.evaluation_strategy.evaluate(invalid_ind, self.toolbox)

                for ind, fit in zip(invalid_ind, fitnesses, strict=True):
                    ind.fitness.values = fit

            return known

        known = update_fitness(individuals)

        # Cr√©ation du logbook (logique inchang√©e)
        individual_params = [list(ind) for ind in individuals]
        parameter_names = list(
            self.cost_function.functional_groups.unique_functional_groups_parameters_ordered().keys()
        )
        fitness_names = [obs.name for obs in self.cost_function.observations]
        fitness_values = [tuple(ind.fitness.values) for ind in individuals]

        logbook = OptimizationLog.from_individual(
            generation=generation,
            is_from_previous_generation=known,
            individual=individual_params,
            parameter_names=parameter_names,
            fitness_names=fitness_names,
        )

        logbook.update_fitness(generation, list(range(len(individuals))), fitness_values)
        return logbook

    def optimize(self: GeneticAlgorithm) -> OptimizationLog:
        """
        Logique d'optimisation pure, sans pr√©occupation de distribution.
        """
        generation_start, population = self._initialization()

        for gen in range(generation_start, self.meta_parameter.NGEN):
            logger.info(f"Generation {gen} / {self.meta_parameter.NGEN}.")

            # S√©lection, croisement, mutation (logique GA standard)
            offspring = self.toolbox.select(population, self.meta_parameter.POP_SIZE)
            offspring = self.meta_parameter.variation(
                offspring, self.toolbox, self.meta_parameter.CXPB, self.meta_parameter.MUTPB
            )

            # √âvaluation d√©l√©gu√©e √† la strat√©gie
            logbook = self._evaluate(offspring, gen)

            self.update_logbook(logbook)
            population[:] = offspring

        return self.logbook.copy()
```

### 4. Factory Pattern pour Simplifier l'Usage

```python
class GeneticAlgorithmFactory:
    """Factory pour cr√©er des instances GeneticAlgorithm configur√©es."""

    @staticmethod
    def create_sequential(meta_parameter: GeneticAlgorithmParameters,
                         cost_function: CostFunctionProtocol,
                         **kwargs) -> GeneticAlgorithm:
        """Cr√©e un GA en mode s√©quentiel."""
        return GeneticAlgorithm(
            meta_parameter=meta_parameter,
            cost_function=cost_function,
            evaluation_strategy=SequentialEvaluation(),
            **kwargs
        )

    @staticmethod
    def create_parallel(meta_parameter: GeneticAlgorithmParameters,
                       cost_function: CostFunctionProtocol,
                       n_jobs: int = -1,
                       **kwargs) -> GeneticAlgorithm:
        """Cr√©e un GA en mode parall√®le multiprocessing."""
        return GeneticAlgorithm(
            meta_parameter=meta_parameter,
            cost_function=cost_function,
            evaluation_strategy=ParallelEvaluation(n_jobs=n_jobs),
            **kwargs
        )

    @staticmethod
    def create_distributed(meta_parameter: GeneticAlgorithmParameters,
                          cost_function: CostFunctionProtocol,
                          client: Client,
                          auto_distribute: bool = True,
                          **kwargs) -> tuple[GeneticAlgorithm, DistributionManager]:
        """
        Cr√©e un GA en mode distribu√© avec Dask.

        Parameters
        ----------
        auto_distribute : bool
            Si True, distribue automatiquement les donn√©es lourdes

        Returns
        -------
        tuple[GeneticAlgorithm, DistributionManager]
            L'instance GA et le gestionnaire de distribution
        """
        # Cr√©er le gestionnaire de distribution
        dist_manager = DistributionManager(client)

        if auto_distribute:
            # Distribution automatique des donn√©es lourdes
            dist_manager.distribute_forcing(cost_function.model_generator.forcing_parameters)
            dist_manager.distribute_observations(cost_function.observations)

        # Cr√©er la strat√©gie d'√©valuation distribu√©e
        evaluation_strategy = DistributedEvaluation(dist_manager)

        # Cr√©er l'instance GA
        ga = GeneticAlgorithm(
            meta_parameter=meta_parameter,
            cost_function=cost_function,
            evaluation_strategy=evaluation_strategy,
            **kwargs
        )

        return ga, dist_manager
```

## üìã Usage Simplifi√©

### Mode S√©quentiel (inchang√©)
```python
ga = GeneticAlgorithmFactory.create_sequential(meta_params, cost_function)
results = ga.optimize()
```

### Mode Parall√®le Classique
```python
ga = GeneticAlgorithmFactory.create_parallel(meta_params, cost_function, n_jobs=4)
results = ga.optimize()
```

### Mode Distribu√© Automatique
```python
client = Client()
ga, dist_manager = GeneticAlgorithmFactory.create_distributed(
    meta_params, cost_function, client, auto_distribute=True
)
results = ga.optimize()
dist_manager.cleanup()
```

### Mode Distribu√© Manuel (contr√¥le fin)
```python
client = Client()
dist_manager = DistributionManager(client)

# Distribution s√©lective
dist_manager.distribute_forcing(forcing_params)
dist_manager.distribute_observations(observations)

evaluation_strategy = DistributedEvaluation(dist_manager)
ga = GeneticAlgorithm(meta_params, cost_function, evaluation_strategy)
results = ga.optimize()
```

## üß™ Structure de Tests

```python
# tests/test_distribution_manager.py
class TestDistributionManager:
    def test_distribute_forcing(self):
        # Test distribution des forcing parameters

    def test_distribute_observations(self):
        # Test distribution des observations

    def test_create_distributed_evaluator(self):
        # Test cr√©ation de l'√©valuateur distribu√©

    def test_cleanup(self):
        # Test nettoyage des ressources

# tests/test_evaluation_strategies.py
class TestEvaluationStrategies:
    def test_sequential_evaluation(self):
        # Test √©valuation s√©quentielle

    def test_distributed_evaluation(self):
        # Test √©valuation distribu√©e

    def test_parallel_evaluation(self):
        # Test √©valuation parall√®le multiprocessing

# tests/test_genetic_algorithm.py
class TestGeneticAlgorithm:
    def test_optimization_logic(self):
        # Test logique GA avec mock evaluation strategy

    def test_strategy_injection(self):
        # Test injection de diff√©rentes strat√©gies

# tests/test_factory.py
class TestGeneticAlgorithmFactory:
    def test_create_sequential(self):
        # Test factory s√©quentiel

    def test_create_distributed(self):
        # Test factory distribu√©

# tests/integration/
test_memory_usage.py      # Tests de consommation m√©moire
test_performance.py       # Tests de performance
test_notebook_examples.py # Tests sur les exemples notebooks
```

## ‚úÖ Avantages de cette Architecture

### üéØ **S√©paration des Responsabilit√©s**
- `GeneticAlgorithm` ‚Üí **Logique m√©tier GA pure**
- `DistributionManager` ‚Üí **Gestion Dask isol√©e**
- `EvaluationStrategy` ‚Üí **Modes d'ex√©cution modulaires**

### üìñ **Lisibilit√© Utilisateur**
- **Focus m√©tier** : Les utilisateurs voient la logique GA principale
- **Distribution optionnelle** : Complexit√© cach√©e quand non utilis√©e
- **Configuration explicite** : Pas de magie noire

### üß™ **Testabilit√© Maximale**
- **Tests isol√©s** par responsabilit√©
- **Mocks faciles** avec injection de strat√©gies
- **Tests d'int√©gration** cibl√©s

### üöÄ **Extensibilit√© Future**
- **Nouveaux backends** : Ray, MPI, Cloud APIs
- **Nouvelles strat√©gies** : GPU computing, edge computing
- **Nouvelles optimisations** : Adaptive scheduling, load balancing

### üîÑ **R√©trocompatibilit√©**
- **API existante** pr√©serv√©e via factory methods
- **Migration progressive** possible
- **Notebooks existants** fonctionnent sans modification

## üéõÔ∏è Plan de Migration

### Phase 1 : Impl√©mentation Base
1. Cr√©er `DistributionManager` et `EvaluationStrategy`
2. Impl√©menter `SequentialEvaluation` et `DistributedEvaluation`
3. Tests unitaires complets

### Phase 2 : Refactoring GeneticAlgorithm
1. Simplifier la classe principale
2. Injection de strat√©gie d'√©valuation
3. Tests d'int√©gration

### Phase 3 : Factory et Documentation
1. Cr√©er `GeneticAlgorithmFactory`
2. Mettre √† jour la documentation
3. Exemples d'usage

### Phase 4 : Migration et Optimisation
1. Migrer les notebooks existants
2. Tests de performance
3. Optimisations sp√©cifiques

Cette architecture rend le code **beaucoup plus maintenable**, **extensible** et **compr√©hensible** pour tous les types d'utilisateurs !